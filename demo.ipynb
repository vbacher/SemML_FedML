{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demo\n",
    "This demo is meant to demonstrate the basic concepts of a federated learning approach. The task used for this demonstration is a very simple function estimation. The setups is very simplified, we wont use different hardware per node and won't train in parallel but sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Leanring Hyperparameter\n",
    "We will use an example in which we have three nodes. In reality $N$ is much larger, in ome cases $N$ is much larger than the amount of trainings data per node. For demonstration purposes $N=3$ will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter federated learning\n",
    "\n",
    "N = 33 # Number of nodes: Cross silo: 3; cross device: 30\n",
    "C = 10 # Number of Nodes used per trainings round Cross silo: C=N ; cross device: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example task\n",
    "We will use the most general task, a fully connected network can solve; function fitting. Since we focus on federated learning and not the network architecture, we will try to fit a very simple polynomial. in our case $f(x)=x^3-5x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter of Example\n",
    "\n",
    "NOISE = True # add noise to trainings data\n",
    "SHUFFLE = True # shuffle some elements between nodes randomly\n",
    "SUPPORT = (-2.8, 2.8) # support of function\n",
    "PITCH = 0.0075 # pitch between equally spaced samples: cross silo: 0.002# ; cross device: 0.0075\n",
    "RANDOM_SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label generating function to be learned\n",
    "\n",
    "def f (x):\n",
    "  return x*x*x -5*x # simple polynomial of third degree\n",
    "\n",
    "# plot function\n",
    "data = np.arange(SUPPORT[0],SUPPORT[1],PITCH)\n",
    "num_data = len(data)\n",
    "print(f\"Number of samples: {num_data}\")\n",
    "label = f(data)\n",
    "\n",
    "# add label noise\n",
    "if NOISE:\n",
    "  np.random.seed(RANDOM_SEED)\n",
    "  add_noise = np.random.normal(0,0.4,data.shape) # The noise level can be modified.\n",
    "  label+=add_noise\n",
    "\n",
    "plt.plot(data, label, color='red')\n",
    "plt.title(\"function to be learned\")\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "We now have data $(x,y)$ where $x$ is the data and $y$ the corresponding label. We now have to distribute the data across our nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create datasets\n",
    "\n",
    "glob_dataset = np.hstack((data[:,np.newaxis], label[:,np.newaxis]))\n",
    "indices = np.argsort(glob_dataset[:,1])\n",
    "glob_dataset = np.asarray([(glob_dataset[i,0],glob_dataset[i,1]) for i in indices])\n",
    "random.seed(RANDOM_SEED)\n",
    "SEEDS = random.sample(range(100),(int)(0.1*len(glob_dataset)))\n",
    "\n",
    "# shuffle\n",
    "if SHUFFLE:\n",
    "  for k in range((int)(0.1*len(glob_dataset))):\n",
    "    np.random.seed(SEEDS[k])\n",
    "    a,b = np.random.randint(0,len(glob_dataset),2)\n",
    "    tmp = glob_dataset[a]\n",
    "    glob_dataset[a]=glob_dataset[b]\n",
    "    glob_dataset[b]=glob_dataset[a]\n",
    "\n",
    "\n",
    "# split dataset in N parts\n",
    "split = np.array_split(glob_dataset, N)\n",
    "for sub_set in split:\n",
    "  plt.scatter(sub_set[::20,0],sub_set[::20,1], marker='o')\n",
    "plt.title(\"scatter plot of splitted dataset\")\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-IIDness of datasplit\n",
    "An identically distributed split would mean, that each split would have been sampled from the same distribution. In this example this would translate to a uniform sampling of datapoints within the support of the function. This is equivalent to randomly assigning an equal amount of data, label pairs to the nodes.\n",
    "This obviously is not the case here. Independent of the Node, since the underlying function is the same on each node, each $x$ is translated to roughly the same $y$ and vice versa. Thus $P_i(y|x)$ and $P_i(x|y)$ is the same for each Node, so we don't have a concept shift or drift. What does vary is the distribution of the labels $P_i(y)$ and the distribution of the feature $P_i(x)$, thus we do have a Prior and Covariance shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our model\n",
    "After we have our function and split our data, we now need a model to approximate the function.\n",
    "Here we will use a very simple feed forward network with two hidden layers and 16 neurons per hidden layer. Since a multiple times continuously differentiable function is approximated, a Sigmoid function is used as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set hyper parameter\n",
    "\n",
    "NEURONS_PER_LAYER = 16 # number of nodes in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define two layer feed forward network\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,n, seed):\n",
    "      super(FFN, self).__init__()\n",
    "      torch.manual_seed(seed)\n",
    "      self.fc1 = nn.Linear(1, n,True)\n",
    "      self.sig1 = nn.Sigmoid()\n",
    "      self.fc2 = nn.Linear(n,n,True)\n",
    "      self.sig2 = nn.Sigmoid()\n",
    "      self.fc3 = nn.Linear(n,1,False)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "      x = self.fc1(x)\n",
    "      x = self.sig1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.sig2(x)\n",
    "      x = self.fc3(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test general ability to learn function\n",
    "This is more or less only meant as a proof of the general ability of the model to fit the entire dataset. It is not connected to federated learning. As for all federated learning approaches, for simplicity we will not use any validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test global model performance\n",
    "\n",
    "glob_model = FFN(NEURONS_PER_LAYER, RANDOM_SEED)\n",
    "glob_model.double()\n",
    "\n",
    "# define training hyper parameter\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = opt.SGD(glob_model.parameters(), lr=0.01, momentum=0.9)\n",
    "NUM_EPOCHS  = 1500\n",
    "\n",
    "## setup trainings pipeline\n",
    "\n",
    "# Training function\n",
    "def train(model, data, labels, max_epochs):\n",
    "  epochs = []\n",
    "  trainloss = []\n",
    "  data = torch.from_numpy(data[:,np.newaxis])\n",
    "  print(f\"data size : {data.shape}\")\n",
    "  labels = torch.from_numpy(labels[:,np.newaxis])\n",
    "  for epoch in tqdm(range(max_epochs)):\n",
    "    epochs.append(epoch)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = loss_fn(pred,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    trainloss.append(loss.detach().numpy())\n",
    "\n",
    "  epochs = np.asarray(epochs)\n",
    "  trainloss = np.asarray(trainloss)\n",
    "  return epochs, trainloss\n",
    "\n",
    "epochs, train_loss = train(glob_model,glob_dataset[:,0],glob_dataset[:,1],NUM_EPOCHS)\n",
    "\n",
    "## plot training stats\n",
    "\n",
    "# plot loss over epochs\n",
    "plt.plot(epochs, train_loss, color='red')\n",
    "plt.title(\"loss over epochs\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "# plot prediction\n",
    "data_test = np.arange(-3,3,0.005)\n",
    "with torch.no_grad():\n",
    "  y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "\n",
    "y_exact = f(data_test)\n",
    "\n",
    "plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "plt.plot(data_test, f(data_test), color='green')\n",
    "plt.title(f\"Ground truth (green) vs. prediction (red)\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-silo Federated learning pipeline\n",
    "This Experiment is meant to demonstrate a so-called cross silo federated learning approach. In this setting the number of trainings samples per node is much larger than the number of nodes. In our case roughly 1000 samples in each of the three nodes. Further we can assume all nodes to be reachable in each training round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fed parameters\n",
    "\n",
    "MAX_ROUNDS = 20 # number of federated learning rounds T\n",
    "MAX_EPOCH = 1000 # number od epochs per round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cross silo federated learning pipeline\n",
    "\n",
    "# params for visualization\n",
    "RESUME_TO_ENDE=False\n",
    "\n",
    "# init global model\n",
    "glob_model = FFN(NEURONS_PER_LAYER, RANDOM_SEED)\n",
    "glob_model.double()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# init local models\n",
    "loc_models = []\n",
    "loc_optimizers = []\n",
    "for k in range(N):\n",
    "  loc_models.append(FFN(NEURONS_PER_LAYER,10).double())\n",
    "  loc_optimizers.append(opt.SGD(loc_models[k].parameters(), lr=0.01, momentum=0.9))\n",
    "\n",
    "# prepare data\n",
    "if type(split[0]) is np.ndarray:\n",
    "  for k in range(N):\n",
    "    split[k]=torch.from_numpy(split[k]) # needs to be a pytorch tensor\n",
    "\n",
    "# define training function\n",
    "def train(model, data, labels, opt, max_epochs):\n",
    "  epochs = []\n",
    "  trainloss = []\n",
    "  for epoch in range(max_epochs):\n",
    "    epochs.append(epoch)\n",
    "    opt.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = loss_fn(pred,labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    trainloss.append(loss.detach().numpy())\n",
    "\n",
    "  epochs = np.asarray(epochs)\n",
    "  trainloss = np.asarray(trainloss)\n",
    "  return epochs, trainloss\n",
    "\n",
    "# test data for visualization\n",
    "data_test = np.arange(-3,3,0.005)\n",
    "y_exact = torch.from_numpy(f(data_test)[:,np.newaxis])\n",
    "loss_glob = []\n",
    "rounds = []\n",
    "\n",
    "# start training rounds\n",
    "for trainings_round in tqdm(range(MAX_ROUNDS)):#tqdm(range(MAX_ROUNDS)):\n",
    "\n",
    "  rounds.append(trainings_round) # for easier visualization\n",
    "\n",
    "  # 1. broadcast global model to local nodes\n",
    "  for k in range(N):\n",
    "    loc_models[k].load_state_dict(glob_model.state_dict())\n",
    "\n",
    "  # 2. train local models\n",
    "  nodes_epoch = []\n",
    "  nodes_loss = []\n",
    "  for loc_dataset, loc_model, loc_opt in zip(split,loc_models, loc_optimizers):\n",
    "\n",
    "    loc_data = loc_dataset[:,0,None]\n",
    "    loc_label = loc_dataset[:,1,None]\n",
    "\n",
    "    loc_epochs, loc_trainloss = train(loc_model,loc_data,loc_label,loc_opt,MAX_EPOCH) # magic happens here\n",
    "\n",
    "    nodes_epoch.append(loc_epochs)\n",
    "    nodes_loss.append(loc_trainloss)\n",
    "\n",
    "  # 3. aggregate models\n",
    "  glob_sd = copy.deepcopy(glob_model.state_dict())\n",
    "  loc_sds = []\n",
    "  for k in range(N):\n",
    "    loc_sds.append(loc_models[k].state_dict())\n",
    "\n",
    "  # simply average weights\n",
    "  for key in glob_sd.keys():\n",
    "    glob_sd[key]= torch.multiply(glob_sd[key],0)\n",
    "    for k in range(N):\n",
    "      glob_sd[key] += torch.multiply(loc_sds[k][key], 1/N)\n",
    "\n",
    "  # 4. update global model\n",
    "  glob_model.load_state_dict(glob_sd)\n",
    "\n",
    "  # calc glob loss\n",
    "  with torch.no_grad():\n",
    "    y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "  round_loss = loss_fn(y_pred,y_exact)\n",
    "  loss_glob.append(round_loss.detach().numpy())\n",
    "\n",
    "\n",
    "  # print output\n",
    "  if RESUME_TO_ENDE is False:\n",
    "    verbose = input(\"show loss and prediction? [Y/N]\")\n",
    "    if verbose == 'Y' or verbose == 'y':\n",
    "      for k, node_epoch, node_loss, loc_model, loc_dataset in zip(range(N),nodes_epoch,nodes_loss, loc_models, split):\n",
    "\n",
    "        # plot loc dataset\n",
    "        plt.scatter(loc_dataset[:,0], loc_dataset[:,1])\n",
    "        plt.xlim(-3,3)\n",
    "        plt.ylim(-12,12)\n",
    "        plt.title(f\"Dataset of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('data')\n",
    "        plt.ylabel('label')\n",
    "        plt.show()\n",
    "\n",
    "        # plot loss\n",
    "        plt.plot(node_epoch, node_loss, color='red')\n",
    "        plt.title(f\"loss over epochs of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "        # plot prediction\n",
    "        with torch.no_grad():\n",
    "          y_pred = loc_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "\n",
    "        plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "        plt.plot(data_test, f(data_test), color='green')\n",
    "        plt.title(f\"prediction of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()\n",
    "      # plot averaged prediction\n",
    "      with torch.no_grad():\n",
    "        y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "      plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "      plt.plot(data_test, f(data_test), color='green')\n",
    "      plt.title(f\"Prediction of glob model in round {trainings_round}\")\n",
    "      plt.xlabel('x')\n",
    "      plt.ylabel('y')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    resume = input(\"resume rest of rounds? [Y/N]\")\n",
    "    if resume == 'Y' or resume=='y':\n",
    "      RESUME_TO_ENDE = True\n",
    "\n",
    "# plot final performance data\n",
    "with torch.no_grad():\n",
    "  y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "plt.plot(data_test, f(data_test), color='green')\n",
    "plt.title(f\"Prediction learned model\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.asarray(rounds),np.asarray(loss_glob))\n",
    "plt.title(f\"global loss over training rounds\")\n",
    "plt.xlabel('training rounds')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross device setting\n",
    "The previous example was, as mentioned before, a cross silo setting. In this setting it is assumed that each node is always online or available and the number of nodes is much smaller than the size of the partitions per node. These assumptions do not hold, when training for example on smartphones. Some might be offline, the number of smartphones in a network by far exceeds the numer of samples per phone and the amount of smartphones makes it unfeasible to use all phones per trainings round. Thus, we have to introduce a new hyper parameter $C$ denoting the number of devices used per round. We also have to adapt some parameters from above. Set $N$ to 30 and $C$ to 10 to get less nodes used per round than available nodes. The pith also needs to get adjusted to 0.0075 since otherwise we won't have less samples per nodes than nodes. Then we have to create a new dataset and split, before we can test the cross device approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:33<01:49,  2.74s/it]"
     ]
    }
   ],
   "source": [
    "### cross device federated learning pipeline\n",
    "\n",
    "# trainings params\n",
    "MAX_ROUNDS = 50 # number of federated learning rounds T\n",
    "MAX_EPOCH = 500 # number od epochs per round\n",
    "random.seed(RANDOM_SEED)\n",
    "SEEDS = random.sample(range(100),MAX_ROUNDS)\n",
    "\n",
    "# params for visualization\n",
    "RESUME_TO_ENDE=False\n",
    "\n",
    "# init global model\n",
    "glob_model = FFN(NEURONS_PER_LAYER, RANDOM_SEED)\n",
    "glob_model.double()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# init local models\n",
    "loc_models = []\n",
    "loc_optimizers = []\n",
    "for k in range(N):\n",
    "  loc_models.append(FFN(NEURONS_PER_LAYER,10).double())\n",
    "  loc_optimizers.append(opt.SGD(loc_models[k].parameters(), lr=0.01, momentum=0.9))\n",
    "\n",
    "# prepare data\n",
    "if type(split[0]) is np.ndarray:\n",
    "  for k in range(N):\n",
    "    split[k]=torch.from_numpy(split[k]) # needs to be a pytorch tensor\n",
    "\n",
    "# define training function\n",
    "def train(model, data, labels, opt, max_epochs):\n",
    "  epochs = []\n",
    "  trainloss = []\n",
    "  for epoch in range(max_epochs):\n",
    "    epochs.append(epoch)\n",
    "    opt.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = loss_fn(pred,labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    trainloss.append(loss.detach().numpy())\n",
    "\n",
    "  epochs = np.asarray(epochs)\n",
    "  trainloss = np.asarray(trainloss)\n",
    "  return epochs, trainloss\n",
    "\n",
    "# test data for visualization\n",
    "data_test = np.arange(-3,3,0.005)\n",
    "y_exact = torch.from_numpy(f(data_test)[:,np.newaxis])\n",
    "loss_glob = []\n",
    "rounds = []\n",
    "min_loss = 10e5\n",
    "\n",
    "# start training rounds\n",
    "for trainings_round in tqdm(range(MAX_ROUNDS)):#tqdm(range(MAX_ROUNDS)):\n",
    "\n",
    "  rounds.append(trainings_round) # for easier visualization\n",
    "\n",
    "  # 1. select clients\n",
    "  random.seed(SEEDS[trainings_round])\n",
    "  selection = random.sample(range(N),C)\n",
    "\n",
    "  # 1. broadcast global model to local nodes\n",
    "  for k in selection:\n",
    "    loc_models[k].load_state_dict(glob_model.state_dict())\n",
    "\n",
    "  # 2. train local models\n",
    "  nodes_epoch = []\n",
    "  nodes_loss = []\n",
    "  loc_sds = []\n",
    "  denom = 0\n",
    "  for k in selection:\n",
    "\n",
    "    # get data of selection\n",
    "    loc_dataset = split[k]\n",
    "    loc_model = loc_models[k]\n",
    "    loc_opt = loc_optimizers[k]\n",
    "\n",
    "    loc_data = loc_dataset[:,0,None]\n",
    "    loc_label = loc_dataset[:,1,None]\n",
    "\n",
    "    loc_epochs, loc_trainloss = train(loc_model,loc_data,loc_label,loc_opt,MAX_EPOCH) # magic happens here\n",
    "\n",
    "    nodes_epoch.append(loc_epochs)\n",
    "    nodes_loss.append(loc_trainloss)\n",
    "    denom += loc_dataset.shape[0]\n",
    "\n",
    "    # 3. aggregate models\n",
    "    loc_sds.append(loc_model.state_dict())\n",
    "\n",
    "  glob_sd = copy.deepcopy(glob_model.state_dict())\n",
    "  nom = (int)(np.log(5*trainings_round*C/MAX_ROUNDS+1))\n",
    "  denom += nom\n",
    "\n",
    "  # simply average weights\n",
    "  for key in glob_sd.keys():\n",
    "    glob_sd[key]= torch.multiply(glob_sd[key],nom/denom)\n",
    "    for i,k in enumerate(selection):\n",
    "      factor = split[k].shape[0]/denom\n",
    "      glob_sd[key] += torch.multiply(loc_sds[i][key], factor)\n",
    "\n",
    "  # 4. update global model\n",
    "  glob_model.load_state_dict(glob_sd)\n",
    "\n",
    "  # calc glob loss\n",
    "  with torch.no_grad():\n",
    "    y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "  round_loss = loss_fn(y_pred,y_exact)\n",
    "  loss_glob.append(round_loss.detach().numpy())\n",
    "  min_loss = min(min_loss,round_loss.detach().numpy())\n",
    "\n",
    "\n",
    "  # print output\n",
    "  if RESUME_TO_ENDE is False:\n",
    "    verbose = input(\"show loss and prediction? [Y/N]\")\n",
    "    if verbose == 'Y' or verbose == 'y':\n",
    "      for i,k in enumerate(selection):\n",
    "        node_epoch = nodes_epoch[i]\n",
    "        node_loss = nodes_loss[i]\n",
    "        loc_model = loc_models[k]\n",
    "        loc_dataset = split[k]\n",
    "\n",
    "        # plot loc dataset\n",
    "        plt.scatter(loc_dataset[:,0], loc_dataset[:,1])\n",
    "        plt.xlim(-3,3)\n",
    "        plt.ylim(-12,12)\n",
    "        plt.title(f\"Dataset of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('data')\n",
    "        plt.ylabel('label')\n",
    "        plt.show()\n",
    "\n",
    "        # plot loss\n",
    "        plt.plot(node_epoch, node_loss, color='red')\n",
    "        plt.title(f\"loss over epochs of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "        # plot prediction\n",
    "        with torch.no_grad():\n",
    "          y_pred = loc_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "\n",
    "        plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "        plt.plot(data_test, f(data_test), color='green')\n",
    "        plt.title(f\"prediction of Node {k} in round {trainings_round}\")\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()\n",
    "      # plot averaged prediction\n",
    "      with torch.no_grad():\n",
    "        y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "      plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "      plt.plot(data_test, f(data_test), color='green')\n",
    "      plt.title(f\"Prediction of glob model in round {trainings_round}\")\n",
    "      plt.xlabel('x')\n",
    "      plt.ylabel('y')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    resume = input(\"resume rest of rounds? [Y/N]\")\n",
    "    if resume == 'Y' or resume=='y':\n",
    "      RESUME_TO_ENDE = True\n",
    "\n",
    "print(f\"minimal global loss: {min_loss}\")\n",
    "\n",
    "# plot final performance data\n",
    "with torch.no_grad():\n",
    "  y_pred = glob_model(torch.from_numpy(data_test[:,np.newaxis]))\n",
    "plt.plot(data_test, y_pred.detach().numpy(), color='red')\n",
    "plt.plot(data_test, f(data_test), color='green')\n",
    "plt.title(f\"Prediction learned model\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.asarray(rounds),np.asarray(loss_glob))\n",
    "plt.title(f\"global loss over training rounds\")\n",
    "plt.xlabel('training rounds')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test a new averaging scheme to smooth the training, I have adapted the averaging step. It does seem to help in some cases increasing the contribution of the global model in the averaging step per epoch, I havent completly tested it thou, so I dont know whether I will include it in the presentation and paper or not."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (SemML_FedML)",
   "language": "python",
   "name": "pycharm-eb53ddbe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
